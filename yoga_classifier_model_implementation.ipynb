{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67b3792e-a73f-4854-aa75-1d3f032ce85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!!\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello World!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18811074-1a18-4b13-b4ea-6667cfc1c582",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7f3a714-324f-45aa-b055-587f7849fe7d",
   "metadata": {},
   "source": [
    "## Installing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "236692cc-a55c-45b6-b7b1-2a5b37633b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in d:\\softwares\\anaconda\\envs\\dlgpu\\lib\\site-packages (2.19.0)\n",
      "Requirement already satisfied: tensorflow-hub in d:\\softwares\\anaconda\\envs\\dlgpu\\lib\\site-packages (0.16.1)\n",
      "Requirement already satisfied: joblib in d:\\softwares\\anaconda\\envs\\dlgpu\\lib\\site-packages (1.5.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in d:\\softwares\\anaconda\\envs\\dlgpu\\lib\\site-packages (from tensorflow) (2.2.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in d:\\softwares\\anaconda\\envs\\dlgpu\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in d:\\softwares\\anaconda\\envs\\dlgpu\\lib\\site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in d:\\softwares\\anaconda\\envs\\dlgpu\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in d:\\softwares\\anaconda\\envs\\dlgpu\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in d:\\softwares\\anaconda\\envs\\dlgpu\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in d:\\softwares\\anaconda\\envs\\dlgpu\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in d:\\softwares\\anaconda\\envs\\dlgpu\\lib\\site-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in d:\\softwares\\anaconda\\envs\\dlgpu\\lib\\site-packages (from tensorflow) (5.29.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in d:\\softwares\\anaconda\\envs\\dlgpu\\lib\\site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in d:\\softwares\\anaconda\\envs\\dlgpu\\lib\\site-packages (from tensorflow) (78.1.1)\n",
      "Requirement already satisfied: six>=1.12.0 in d:\\softwares\\anaconda\\envs\\dlgpu\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in d:\\softwares\\anaconda\\envs\\dlgpu\\lib\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in d:\\softwares\\anaconda\\envs\\dlgpu\\lib\\site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in d:\\softwares\\anaconda\\envs\\dlgpu\\lib\\site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in d:\\softwares\\anaconda\\envs\\dlgpu\\lib\\site-packages (from tensorflow) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in d:\\softwares\\anaconda\\envs\\dlgpu\\lib\\site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in d:\\softwares\\anaconda\\envs\\dlgpu\\lib\\site-packages (from tensorflow) (3.9.2)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in d:\\softwares\\anaconda\\envs\\dlgpu\\lib\\site-packages (from tensorflow) (2.1.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in d:\\softwares\\anaconda\\envs\\dlgpu\\lib\\site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in d:\\softwares\\anaconda\\envs\\dlgpu\\lib\\site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in d:\\softwares\\anaconda\\envs\\dlgpu\\lib\\site-packages (from tensorflow) (0.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\softwares\\anaconda\\envs\\dlgpu\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\softwares\\anaconda\\envs\\dlgpu\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\softwares\\anaconda\\envs\\dlgpu\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\softwares\\anaconda\\envs\\dlgpu\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
      "Requirement already satisfied: markdown>=2.6.8 in d:\\softwares\\anaconda\\envs\\dlgpu\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.8)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in d:\\softwares\\anaconda\\envs\\dlgpu\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in d:\\softwares\\anaconda\\envs\\dlgpu\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: tf-keras>=2.14.1 in d:\\softwares\\anaconda\\envs\\dlgpu\\lib\\site-packages (from tensorflow-hub) (2.19.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in d:\\softwares\\anaconda\\envs\\dlgpu\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in d:\\softwares\\anaconda\\envs\\dlgpu\\lib\\site-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n",
      "Requirement already satisfied: namex in d:\\softwares\\anaconda\\envs\\dlgpu\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
      "Requirement already satisfied: optree in d:\\softwares\\anaconda\\envs\\dlgpu\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in d:\\softwares\\anaconda\\envs\\dlgpu\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\softwares\\anaconda\\envs\\dlgpu\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\softwares\\anaconda\\envs\\dlgpu\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\softwares\\anaconda\\envs\\dlgpu\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow tensorflow-hub joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d76460-4c18-4df6-871d-a89a8223c296",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Running the model for single person (very latent)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "27ad61fd-424a-45e3-8eed-4821bfb14306",
   "metadata": {},
   "source": [
    "Simple execution only on single person ( able to detect pose with latency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d176dcba-31c8-489d-b809-0482a95f5b88",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaba3f72-e339-492d-9cc8-2723b3fb8c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“¦ Imports\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46fb8f9-f771-468b-b8f3-1fea019492d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f72db8dd-7544-403d-9026-0ff48518de0a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430e1925-4264-46f3-83a5-bb519521adfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Load model & label encoder\n",
    "model = tf.keras.models.load_model(\"yoga_pose_nn.h5\")\n",
    "label_encoder = joblib.load(\"label_encoder.pkl\")\n",
    "\n",
    "# ðŸ”„ Load MoveNet model from TF Hub\n",
    "movenet = hub.load(\"https://tfhub.dev/google/movenet/singlepose/thunder/4\")\n",
    "\n",
    "def detect_keypoints_from_frame(frame):\n",
    "    \"\"\"Extract 17 keypoints (x, y) from an RGB frame using MoveNet.\"\"\"\n",
    "    img = tf.image.resize_with_pad(tf.convert_to_tensor(frame), 256, 256)\n",
    "    input_img = tf.expand_dims(tf.cast(img, dtype=tf.int32), axis=0)\n",
    "    keypoints = movenet.signatures['serving_default'](input_img)['output_0'].numpy()\n",
    "    return keypoints[0, 0, :, :2].flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd747d2-d9e4-4133-b145-d95c11a1808b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7d458b7-2dfd-4b47-a36c-3d6fa9c5400f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Checking in real-time working on single person"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cf9dbc87-f052-4696-9307-3454dbd1800b",
   "metadata": {},
   "source": [
    "The below code works only on single person in the img and also there is a lot of latency in the output video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93309837-5630-48ef-9090-a351ec16aade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŽ¥ Start webcam\n",
    "cap = cv2.VideoCapture('yog.mp4')\n",
    "print(\"Press 'q' to quit.\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    try:\n",
    "        keypoints = detect_keypoints_from_frame(frame_rgb)\n",
    "        keypoints = np.expand_dims(keypoints, axis=0)\n",
    "        prediction = model.predict(keypoints)\n",
    "        class_id = np.argmax(prediction)\n",
    "        label = label_encoder.inverse_transform([class_id])[0]\n",
    "\n",
    "        cv2.putText(frame, f\"Pose: {label}\", (10, 40),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 3)\n",
    "    except Exception as e:\n",
    "        cv2.putText(frame, \"Pose: Not detected\", (10, 40),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 3)\n",
    "\n",
    "    cv2.imshow(\"Yoga Pose Classifier\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a43bb5d-c913-402d-965e-7915fb74d925",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe0327c4-1721-4817-8b46-98646838add2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Running the model for multi person using yolo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867347b9-9d70-426e-b9e7-0801e771fe3b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f46934-c0f0-4246-b539-40521baeb91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow tensorflow-hub opencv-python joblib ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6841cd-0b9c-4135-80cf-0b3fbc010a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import joblib\n",
    "from ultralytics import YOLO\n",
    "\n",
    "print(\"Cell Executed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584557e4-b00b-4165-9200-443c6128644a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d684363-441f-4063-8daa-462c7a5aa781",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d87fc27-db98-4734-9d2b-3ac0f3dad63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YOLOv8 model (pretrained on COCO)\n",
    "yolo_model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "# Load MoveNet and classifier\n",
    "movenet = hub.load(\"https://tfhub.dev/google/movenet/singlepose/thunder/4\")\n",
    "pose_model = tf.keras.models.load_model(\"yoga_pose_nn.h5\")\n",
    "label_encoder = joblib.load(\"label_encoder.pkl\")\n",
    "\n",
    "# Helper to extract keypoints\n",
    "def detect_keypoints_from_crop(crop):\n",
    "    img = tf.image.resize_with_pad(tf.convert_to_tensor(crop), 256, 256)\n",
    "    input_img = tf.expand_dims(tf.cast(img, dtype=tf.int32), axis=0)\n",
    "    keypoints = movenet.signatures['serving_default'](input_img)['output_0'].numpy()\n",
    "    return keypoints[0, 0, :, :2].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a15cf7b-7ba9-45f0-b47a-b4ee9ad7d880",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f7f7a4a-ccb4-4a81-a92b-c2f0dc8b49ae",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Model checking on video"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a75edd79-33b3-4a54-b8fb-d1439063dcbd",
   "metadata": {},
   "source": [
    "Very buggy code, just trash, a huge latency can't go with it.\n",
    "For multi person, it works but there is a lot of latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdf2247-0ff0-4c17-9c69-653ed201f774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start webcam\n",
    "cap = cv2.VideoCapture('my.mp4')\n",
    "print(\"Press 'q' to quit.\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # YOLOv8 inference\n",
    "    results = yolo_model(frame)[0]\n",
    "    boxes = results.boxes.xyxy.cpu().numpy()\n",
    "    classes = results.boxes.cls.cpu().numpy()\n",
    "\n",
    "    for i, cls in enumerate(classes):\n",
    "        if int(cls) != 0:  # 0 = person class in COCO\n",
    "            continue\n",
    "\n",
    "        x1, y1, x2, y2 = map(int, boxes[i])\n",
    "        person_crop = frame[y1:y2, x1:x2]\n",
    "\n",
    "        try:\n",
    "            keypoints = detect_keypoints_from_crop(person_crop)\n",
    "            prediction = pose_model.predict(np.expand_dims(keypoints, axis=0))\n",
    "            class_id = np.argmax(prediction)\n",
    "            label = label_encoder.inverse_transform([class_id])[0]\n",
    "            confidence = np.max(prediction)\n",
    "\n",
    "            # Draw box + label\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"{label} ({confidence:.2f})\", (x1, y1 - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "        except Exception as e:\n",
    "            cv2.putText(frame, \"Pose not detected\", (x1, y1 - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
    "\n",
    "    # Add this right before cv2.imshow()\n",
    "    resized_frame = cv2.resize(\n",
    "        frame, \n",
    "        (1280, 720),  # (width, height)\n",
    "        interpolation=cv2.INTER_LINEAR  # Use INTER_AREA for downsizing\n",
    "    )\n",
    "    cv2.imshow(\"Multi-Person Yoga Pose Classifier\", resized_frame)\n",
    "\n",
    "    # cv2.imshow(\"Multi-Person Yoga Pose Classifier\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f89fed-75f1-4554-9ec7-b40363bcc66e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fef72f7a-0b6d-416b-8b27-f00609ed0557",
   "metadata": {},
   "source": [
    "# Reducing latency code for multi person"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc4dc79-4b8a-4ebb-888b-1f58be6c1c66",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Check if GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57f7d4be-575d-45a1-9311-6896038a1a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "import torch\n",
    "print(torch.cuda.is_available())  # Should print True if GPU is detected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2fa8505-280d-4082-a76a-18f7bcb35bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available: 0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d46bc1f-8ff7-45ea-a7ef-7c011a8925cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available? True\n",
      "Device: NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Is CUDA available?\", torch.cuda.is_available())\n",
    "print(\"Device:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c63b6f6-1f36-433d-a7fe-02a30efec8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPU:  1\n",
      "GPU Name:  NVIDIA GeForce GTX 1650\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Number of GPU: \", torch.cuda.device_count())\n",
    "print(\"GPU Name: \", torch.cuda.get_device_name())\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb014dda-5503-4963-a6c2-707e492ebf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e90a62b-6746-4e76-8294-6da36d37133d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee6333b1-c8a8-4e3e-b8a9-4315ee140eb4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Example Code to check optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dfce62-5790-40b0-bbf9-182b7e583df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import threading\n",
    "import queue\n",
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd52f36a-ca33-4bcc-884c-c8eb44818c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ultralytics import YOLO  # Install ultralytics first\n",
    "\n",
    "# Load model (correct method for YOLOv8+)\n",
    "yolo_model = YOLO('yolov8n.pt')  # Automatically uses GPU if available\n",
    "\n",
    "print(\"Executed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9f8136-5678-4de0-897b-79b056d2a717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MoveNet Thunder from TF Hub\n",
    "movenet = hub.load(\"https://tfhub.dev/google/movenet/singlepose/thunder/4\")\n",
    "input_size = 256\n",
    "\n",
    "# Dummy functions to load your pose classification model and label encoder\n",
    "# Replace with your actual model loading code!\n",
    "def load_pose_model():\n",
    "    # Example: TensorFlow/Keras model\n",
    "    from tensorflow.keras.models import load_model\n",
    "    return load_model(\"yoga_pose_nn.h5\")\n",
    "\n",
    "def load_label_encoder():\n",
    "    import joblib\n",
    "    return joblib.load(\"label_encoder.pkl\")\n",
    "\n",
    "pose_model = load_pose_model()\n",
    "label_encoder = load_label_encoder()\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4aa190-cf96-43a3-a366-f572bc7ce52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Helper functions ===\n",
    "\n",
    "def detect_keypoints_from_crop(crop_img):\n",
    "    # Preprocess crop for MoveNet\n",
    "    img = tf.image.resize_with_pad(tf.expand_dims(crop_img, axis=0), input_size, input_size)\n",
    "    img = tf.cast(img, dtype=tf.int32)\n",
    "    \n",
    "    # Run MoveNet\n",
    "    outputs = movenet.signatures['serving_default'](img)\n",
    "    keypoints = outputs['output_0'].numpy()\n",
    "    keypoints = keypoints[0, 0, :, :2].flatten()  # (17*2,)\n",
    "    return keypoints\n",
    "\n",
    "print(\"Executed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0f889a-e14b-4881-83c5-e49979909b40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "d1a70a95-28dd-4612-ab5e-1afb56804312",
   "metadata": {},
   "source": [
    "Works on multiperson but again very slow --- no use code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e61392-f22b-4119-8613-bc7afdb74185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Video Capture Thread ===\n",
    "\n",
    "frame_queue = queue.Queue(maxsize=5)\n",
    "stop_signal = False\n",
    "\n",
    "def video_capture_thread(video_path=0):  # 0 for webcam, or filename for video\n",
    "    global stop_signal\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    while not stop_signal:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if not frame_queue.full():\n",
    "            frame_queue.put(frame)\n",
    "    cap.release()\n",
    "\n",
    "# === Main Processing Loop ===\n",
    "\n",
    "def main(video_path=0):\n",
    "    global stop_signal\n",
    "\n",
    "    # Start video capture thread\n",
    "    threading.Thread(target=video_capture_thread, args=(video_path,), daemon=True).start()\n",
    "\n",
    "    frame_skip = 2  # skip frames to speed up, adjust as needed\n",
    "    frame_count = 0\n",
    "\n",
    "    while True:\n",
    "        if not frame_queue.empty():\n",
    "            frame = frame_queue.get()\n",
    "\n",
    "            if frame_count % frame_skip != 0:\n",
    "                frame_count += 1\n",
    "                continue\n",
    "            frame_count += 1\n",
    "\n",
    "            # Resize for faster YOLO inference\n",
    "            small_frame = cv2.resize(frame, (640, 360))\n",
    "            \n",
    "            # YOLO expects images in RGB\n",
    "            small_frame_rgb = cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Run YOLO on GPU\n",
    "            results = yolo_model(small_frame_rgb)[0]\n",
    "\n",
    "            scale_x = frame.shape[1] / 640\n",
    "            scale_y = frame.shape[0] / 360\n",
    "\n",
    "            boxes = results.boxes.xyxy.cpu().numpy() * [scale_x, scale_y, scale_x, scale_y]\n",
    "            boxes = boxes.astype(int)\n",
    "            classes = results.boxes.cls.cpu().numpy()\n",
    "\n",
    "            for i, cls in enumerate(classes):\n",
    "                if int(cls) != 0:\n",
    "                    continue  # Only person class\n",
    "\n",
    "                x1, y1, x2, y2 = boxes[i]\n",
    "                person_crop = frame[y1:y2, x1:x2]\n",
    "\n",
    "                try:\n",
    "                    keypoints = detect_keypoints_from_crop(person_crop)\n",
    "                    prediction = pose_model.predict(np.expand_dims(keypoints, axis=0))\n",
    "                    class_id = np.argmax(prediction)\n",
    "                    label = label_encoder.inverse_transform([class_id])[0]\n",
    "                    confidence = np.max(prediction)\n",
    "\n",
    "                    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                    cv2.putText(frame, f\"{label} ({confidence:.2f})\", (x1, y1 - 10),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "                except Exception as e:\n",
    "                    cv2.putText(frame, \"Pose not detected\", (x1, y1 - 10),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
    "\n",
    "            cv2.imshow(\"Multi-Person Yoga Pose Classifier\", frame)\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                stop_signal = True\n",
    "                break\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# === Run ===\n",
    "if __name__ == \"__main__\":\n",
    "    main(\"my.mp4\")  # 0 for webcam, or replace with filename like \"myvideo.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592ead19-689a-4354-a5e2-8614b8a035c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edca442a-1629-4389-afd1-b556f7aa909d",
   "metadata": {},
   "source": [
    "## Using GPU for better performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a639a6-501c-41eb-aaa0-5f8a833ddbcb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### No latency but can't detect the pose "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2c022f1b-73b7-4990-85e8-10b549bacaed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "from ultralytics import YOLO\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Load YOLOv8\n",
    "yolo_model = YOLO('yolov8n.pt')  # or yolov8s.pt for better accuracy\n",
    "\n",
    "# Load MoveNet\n",
    "movenet = hub.load(\"https://tfhub.dev/google/movenet/singlepose/thunder/4\")\n",
    "input_size = 256\n",
    "\n",
    "# Load trained pose classifier (Neural Network model)\n",
    "pose_model = tf.keras.models.load_model(\"yoga_pose_nn.h5\")\n",
    "label_encoder = joblib.load(\"label_encoder.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002ac4d2-1f16-4708-9b67-9cab6c304a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Helper to extract keypoints from cropped person image\n",
    "def detect_keypoints_from_crop(image):\n",
    "    img = cv2.resize(image, (input_size, input_size))\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    input_img = tf.expand_dims(img, axis=0)\n",
    "    outputs = movenet.signatures['serving_default'](input_img)\n",
    "    keypoints = outputs['output_0'].numpy()[0, 0, :, :2]\n",
    "    return keypoints.flatten()\n",
    "\n",
    "# Threaded per-person inference\n",
    "def process_person(crop, box, results_list):\n",
    "    try:\n",
    "        keypoints = detect_keypoints_from_crop(crop)\n",
    "        prediction = pose_model.predict(np.expand_dims(keypoints, axis=0), verbose=0)\n",
    "        class_id = np.argmax(prediction)\n",
    "        label = label_encoder.inverse_transform([class_id])[0]\n",
    "        confidence = np.max(prediction)\n",
    "        results_list.append((box, label, confidence))\n",
    "    except Exception as e:\n",
    "        results_list.append((box, \"Pose Not Detected\", 0.0))\n",
    "\n",
    "def main(video_path=\"yog.mp4\", save_output=True):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    print(\"Starting video stream... Press 'q' to quit.\")\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    out = None\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Failed to open video.\")\n",
    "        return\n",
    "\n",
    "    executor = ThreadPoolExecutor(max_workers=6)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        results = yolo_model(frame)[0]\n",
    "        boxes = results.boxes.xyxy.cpu().numpy().astype(int)\n",
    "        classes = results.boxes.cls.cpu().numpy().astype(int)\n",
    "\n",
    "        tasks = []\n",
    "        results_list = []\n",
    "\n",
    "        for i, cls in enumerate(classes):\n",
    "            if cls != 0:  # Only person class\n",
    "                continue\n",
    "            x1, y1, x2, y2 = boxes[i]\n",
    "            crop = frame[y1:y2, x1:x2]\n",
    "            tasks.append(executor.submit(process_person, crop, (x1, y1, x2, y2), results_list))\n",
    "\n",
    "        # Wait for all threads to finish\n",
    "        for task in tasks:\n",
    "            task.result()\n",
    "\n",
    "        # Draw results\n",
    "        for box, label, conf in results_list:\n",
    "            x1, y1, x2, y2 = box\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"{label} ({conf:.2f})\", (x1, y1 - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "\n",
    "        # Resize frame for display/output\n",
    "        frame = cv2.resize(frame, (1280, 720))\n",
    "\n",
    "        # Save output if enabled\n",
    "        if save_output:\n",
    "            if out is None:\n",
    "                os.makedirs(\"outputs\", exist_ok=True)\n",
    "                out = cv2.VideoWriter(\"outputs/predicted_output.avi\", fourcc, 20.0, (1280, 720))\n",
    "            out.write(frame)\n",
    "\n",
    "        cv2.imshow(\"Yoga Pose Classifier\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    if out:\n",
    "        out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"Inference complete. Output saved to outputs/predicted_output.avi\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f474870-d6d1-4d6f-b683-7ca1ed298fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "main(\"yog.mp4\")  # Replace with 0 for webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2726974-0777-4de4-bdf7-b606e82764f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f22045a7-1978-4bfc-ba40-77d260de9315",
   "metadata": {},
   "source": [
    "### Little latency but detecting pose good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291e430e-c2ed-4578-b5f4-5b3604e03e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from ultralytics import YOLO\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import threading\n",
    "import joblib\n",
    "\n",
    "# Load models\n",
    "yolo_model = YOLO(\"yolov8n.pt\")\n",
    "movenet = hub.load(\"https://tfhub.dev/google/movenet/singlepose/thunder/4\")\n",
    "pose_model = tf.keras.models.load_model(\"yoga_pose_nn.h5\")\n",
    "label_encoder = joblib.load(\"label_encoder.pkl\")\n",
    "\n",
    "input_size = 256  # Thunder model requires 256x256 input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "95c96012-41f7-464b-9d0e-70f4661e81ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_keypoints_from_crop(crop_img):\n",
    "    # Convert to TensorFlow tensor and maintain aspect ratio\n",
    "    img_tensor = tf.convert_to_tensor(cv2.cvtColor(crop_img, cv2.COLOR_BGR2RGB))\n",
    "    img_tensor = tf.image.resize_with_pad(img_tensor, input_size, input_size)\n",
    "    \n",
    "    # Convert to int32 with 0-255 range (CRITICAL FIX)\n",
    "    img_tensor = tf.cast(img_tensor, dtype=tf.int32)\n",
    "    \n",
    "    # Add batch dimension and run inference\n",
    "    input_image = tf.expand_dims(img_tensor, axis=0)\n",
    "    outputs = movenet.signatures['serving_default'](input_image)\n",
    "    \n",
    "    # Get normalized keypoints and convert to pixel coordinates\n",
    "    keypoints = outputs['output_0'].numpy()[0, 0]  # Shape: (17, 3)\n",
    "    \n",
    "    # Denormalize to crop coordinates\n",
    "    h, w = crop_img.shape[:2]\n",
    "    keypoints[:, 0] *= h  # y-coordinates\n",
    "    keypoints[:, 1] *= w  # x-coordinates\n",
    "    \n",
    "    return keypoints[:, :2].flatten()  # Return (34,) array of (x,y) pairs\n",
    "\n",
    "def classify_pose(crop_img):\n",
    "    try:\n",
    "        keypoints = detect_keypoints_from_crop(crop_img)\n",
    "        # Normalize keypoints for classifier\n",
    "        normalized_kps = keypoints / np.array([crop_img.shape[1], crop_img.shape[0]] * 17)\n",
    "        prediction = pose_model.predict(np.expand_dims(normalized_kps, axis=0), verbose=0)\n",
    "        class_id = np.argmax(prediction)\n",
    "        label = label_encoder.inverse_transform([class_id])[0]\n",
    "        confidence = np.max(prediction)\n",
    "        return label, confidence\n",
    "    except Exception as e:\n",
    "        return \"Pose not detected\", 0\n",
    "\n",
    "def process_person(frame, box, results_list, idx):\n",
    "    x1, y1, x2, y2 = map(int, box)\n",
    "    crop = frame[y1:y2, x1:x2]\n",
    "    label, confidence = classify_pose(crop)\n",
    "    results_list[idx] = (label, confidence, (x1, y1, x2, y2))\n",
    "\n",
    "def process_frame(frame):\n",
    "    results = yolo_model(frame)[0]\n",
    "    boxes = results.boxes.xyxy.cpu().numpy()\n",
    "    classes = results.boxes.cls.cpu().numpy().astype(int)\n",
    "\n",
    "    threads = []\n",
    "    results_list = [None] * len(boxes)\n",
    "\n",
    "    for i, cls in enumerate(classes):\n",
    "        if cls != 0:\n",
    "            continue\n",
    "        thread = threading.Thread(target=process_person, args=(frame, boxes[i], results_list, i))\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "    for result in results_list:\n",
    "        if result is None:\n",
    "            continue\n",
    "        label, confidence, (x1, y1, x2, y2) = result\n",
    "        color = (0, 255, 0) if label != \"Pose not detected\" else (0, 0, 255)\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "        cv2.putText(frame, f\"{label} ({confidence:.2f})\", (x1, y1 - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "    return frame\n",
    "\n",
    "def main(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = None\n",
    "    \n",
    "    print(\"Press 'q' to quit.\")\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "    \n",
    "        frame = process_frame(frame)\n",
    "        frame = cv2.resize(frame, (1280, 720))\n",
    "    \n",
    "        if out is None:\n",
    "            fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "            out = cv2.VideoWriter(\"output_pose.mp4\", fourcc, fps, (1280, 720))\n",
    "        out.write(frame)\n",
    "    \n",
    "        cv2.imshow(\"Yoga Pose Detection\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ee22a490-3145-4154-8496-7fa15a6fb9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 'q' to quit.\n",
      "\n",
      "0: 384x640 1 person, 89.4ms\n",
      "Speed: 247.2ms preprocess, 89.4ms inference, 503.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 61.0ms\n",
      "Speed: 10.1ms preprocess, 61.0ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 61.4ms\n",
      "Speed: 11.3ms preprocess, 61.4ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 60.5ms\n",
      "Speed: 8.7ms preprocess, 60.5ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "main(\"yog.mp4\")  # Or use 0 for webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bab585-5140-4f56-89d0-51ea51e7144d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8ac8018-9eac-4692-a6e1-b262410cf155",
   "metadata": {},
   "source": [
    "### Using lite movenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3641158e-3235-4654-922c-c78d43a6eebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old: Using TensorFlow Hub\n",
    "# movenet = hub.load(\"https://tfhub.dev/google/movenet/singlepose/thunder/4\")\n",
    "\n",
    "# New: Using TensorFlow Lite\n",
    "import os\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "\n",
    "TFLITE_MODEL_URL = \"https://tfhub.dev/google/lite-model/movenet/singlepose/thunder/tflite/float16/4?lite-format=tflite\"\n",
    "TFLITE_MODEL_PATH = \"movenet_thunder.tflite\"\n",
    "\n",
    "if not os.path.exists(TFLITE_MODEL_PATH):\n",
    "    print(\"Downloading MoveNet Thunder TFLite model...\")\n",
    "    r = requests.get(TFLITE_MODEL_URL)\n",
    "    with open(TFLITE_MODEL_PATH, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=TFLITE_MODEL_PATH)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_size = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9dc0d14a-cbbb-4598-af9c-3153d0925340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_keypoints_from_crop(crop_img):\n",
    "    # Preprocess\n",
    "    img_rgb = cv2.cvtColor(crop_img, cv2.COLOR_BGR2RGB)\n",
    "    img_resized = tf.image.resize_with_pad(img_rgb, input_size, input_size)\n",
    "    input_image = tf.cast(tf.expand_dims(img_resized, axis=0), dtype=tf.uint8)  # uint8 for TFLite\n",
    "\n",
    "    # TFLite inference\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_image.numpy())\n",
    "    interpreter.invoke()\n",
    "    keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])  # [1, 1, 17, 3]\n",
    "\n",
    "    # Convert to crop coordinates\n",
    "    keypoints = keypoints_with_scores[0, 0, :, :]  # (17, 3)\n",
    "    h, w = crop_img.shape[:2]\n",
    "    keypoints[:, 0] *= h  # y\n",
    "    keypoints[:, 1] *= w  # x\n",
    "\n",
    "    return keypoints[:, :2].flatten()  # (34,)\n",
    "\n",
    "\n",
    "def classify_pose(crop_img):\n",
    "    try:\n",
    "        keypoints = detect_keypoints_from_crop(crop_img)\n",
    "        # Normalize keypoints for classifier\n",
    "        normalized_kps = keypoints / np.array([crop_img.shape[1], crop_img.shape[0]] * 17)\n",
    "        prediction = pose_model.predict(np.expand_dims(normalized_kps, axis=0), verbose=0)\n",
    "        class_id = np.argmax(prediction)\n",
    "        label = label_encoder.inverse_transform([class_id])[0]\n",
    "        confidence = np.max(prediction)\n",
    "        return label, confidence\n",
    "    except Exception as e:\n",
    "        return \"Pose not detected\", 0\n",
    "\n",
    "def process_person(frame, box, results_list):\n",
    "    x1, y1, x2, y2 = map(int, box)\n",
    "    crop = frame[y1:y2, x1:x2]\n",
    "    label, confidence = classify_pose(crop)\n",
    "    results_list.append((label, confidence, (x1, y1, x2, y2)))\n",
    "\n",
    "def process_frame(frame):\n",
    "    results = yolo_model(frame)[0]\n",
    "    boxes = results.boxes.xyxy.cpu().numpy()\n",
    "    classes = results.boxes.cls.cpu().numpy().astype(int)\n",
    "\n",
    "    threads = []\n",
    "    results_list = []\n",
    "\n",
    "    for i, cls in enumerate(classes):\n",
    "        if cls != 0:\n",
    "            continue\n",
    "        thread = threading.Thread(\n",
    "            target=process_person,\n",
    "            args=(frame, boxes[i], results_list)\n",
    "        )\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "    # Draw results\n",
    "    for label, confidence, (x1, y1, x2, y2) in results_list:\n",
    "        color = (0, 255, 0) if label != \"Pose not detected\" else (0, 0, 255)\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "        cv2.putText(frame, f\"{label} ({confidence:.2f})\", (x1, y1 - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "    return frame\n",
    "\n",
    "def main(video_path=\"yog.mp4\", save_output=True):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    print(\"Starting video stream... Press 'q' to quit.\")\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    out = None\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Failed to open video.\")\n",
    "        return\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = process_frame(frame)\n",
    "        frame = cv2.resize(frame, (1280, 720))\n",
    "\n",
    "        # Save output if enabled\n",
    "        if save_output:\n",
    "            if out is None:\n",
    "                os.makedirs(\"outputs\", exist_ok=True)\n",
    "                out = cv2.VideoWriter(\"outputs/predicted_output.avi\", fourcc, 20.0, (1280, 720))\n",
    "            out.write(frame)\n",
    "\n",
    "        cv2.imshow(\"Yoga Pose Classifier\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    if out:\n",
    "        out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"Inference complete. Output saved to outputs/predicted_output.avi\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccbd0cc4-ad9e-43f9-827e-26c23eeb5d2c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myog.mp4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 73\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(video_path, save_output)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmain\u001b[39m(video_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myog.mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m, save_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m---> 73\u001b[0m     cap \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241m.\u001b[39mVideoCapture(video_path)\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting video stream... Press \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to quit.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     76\u001b[0m     fourcc \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mVideoWriter_fourcc(\u001b[38;5;241m*\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXVID\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main(\"yog.mp4\")  # Replace with your actual video file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630f5ca4-e5fd-4cb1-a868-4ae4e90d4ae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a957e5ed-c6c3-4e59-a186-12bb7653c908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "acf759bc-2b9b-4424-a601-f6a05b6d6538",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Both for Img and Vid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e49b2f7d-c99e-475c-8418-bb57f70a6182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image_path, display=True, save_path=None):\n",
    "    \"\"\"\n",
    "    Process single image for yoga pose detection\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to input image\n",
    "        display (bool): Whether to show the result\n",
    "        save_path (str): Optional path to save result image\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Processed image with annotations\n",
    "    \"\"\"\n",
    "    # Read image\n",
    "    frame = cv2.imread(image_path)\n",
    "    if frame is None:\n",
    "        raise ValueError(f\"Could not read image at {image_path}\")\n",
    "    \n",
    "    # Process frame\n",
    "    processed_frame = process_frame(frame)\n",
    "    \n",
    "    # Resize for display\n",
    "    processed_frame = cv2.resize(processed_frame, (1280, 720))\n",
    "    \n",
    "    # Save/output results\n",
    "    if save_path:\n",
    "        cv2.imwrite(save_path, processed_frame)\n",
    "        print(f\"Saved result to {save_path}\")\n",
    "    \n",
    "    if display:\n",
    "        cv2.imshow(\"Yoga Pose Detection\", processed_frame)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "    \n",
    "    return processed_frame\n",
    "\n",
    "def process_video(video_path, save_output=True):\n",
    "    \"\"\"\n",
    "    Process video file or webcam stream\n",
    "    \n",
    "    Args:\n",
    "        video_path (str/int): Path to video file or 0 for webcam\n",
    "        save_path (str): Optional path to save output video\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(f\"Could not open video source {video_path}\")\n",
    "\n",
    "    # Get video properties\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # Initialize writer\n",
    "    if save_output:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "        out = cv2.VideoWriter(\"output.mp4\", fourcc, fps, (1280, 720))\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Process and resize frame\n",
    "        processed = process_frame(frame)\n",
    "        processed = cv2.resize(processed, (1280, 720))\n",
    "\n",
    "        # Save/show results\n",
    "        if save_output:\n",
    "            out.write(processed)\n",
    "        \n",
    "        cv2.imshow(\"Yoga Pose Detection\", processed)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    if save_output:\n",
    "        out.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e97775de-ae4d-4202-9e19-d3b453f01086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unified interface\n",
    "def yoga_pose_detection(input_path, display=True, save_output=False):\n",
    "    \"\"\"\n",
    "    Main interface for yoga pose detection\n",
    "    \n",
    "    Args:\n",
    "        input_path (str/int): Image path, video path, or 0 for webcam\n",
    "        display (bool): Whether to show results\n",
    "        save_output (bool/str): True to save with default path, or custom path\n",
    "    \"\"\"\n",
    "    if isinstance(input_path, str):\n",
    "        if input_path.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            save_path = \"output.jpg\" if save_output else None\n",
    "            return process_image(input_path, display, save_path)\n",
    "        else:\n",
    "            save_path = \"output_video.mp4\" if save_output else None\n",
    "            process_video(input_path, save_path)\n",
    "    elif isinstance(input_path, int):  # Webcam\n",
    "        process_video(input_path, save_output)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid input type. Use image path, video path, or 0 for webcam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25061328-ec2b-4a94-af2e-61d6b296b4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 448x640 1 person, 1 bottle, 1 chair, 1 couch, 3 potted plants, 1 vase, 455.2ms\n",
      "Speed: 168.5ms preprocess, 455.2ms inference, 1162.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Saved result to output.jpg\n"
     ]
    }
   ],
   "source": [
    "yoga_pose_detection(\"j.jpeg\", True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c3e7e8-9736-4455-9778-0975d2071b3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
